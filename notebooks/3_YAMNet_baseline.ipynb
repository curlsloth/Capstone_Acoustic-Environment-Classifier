{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e748249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 13:47:30.121318: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 13:47:34.788172: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f92587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan data directories\n",
    "import glob\n",
    "\n",
    "nature_file_list = []\n",
    "nature_file_list += glob.glob('../data/interim/AmbisonicSoundLibrary/nature/*')\n",
    "nature_file_list += glob.glob('../data/interim/GoogleAudioSet/Outside, rural or natural/*')\n",
    "nature_file_list += glob.glob('../data/interim/youtube/NatureSoundscapes/*')\n",
    "nature_file_list += glob.glob('../data/interim/youtube/NomadicAmbience_nature/*')\n",
    "nature_file_list += glob.glob('../data/interim/S2L_LULC/non_urban/*')\n",
    "nature_file_list += glob.glob('../data/interim/S2L_LULC/urban_0_25/*')\n",
    "\n",
    "city_file_list = []\n",
    "city_file_list += glob.glob('../data/interim/GoogleAudioSet/Outside, urban or manmade/*')\n",
    "city_file_list += glob.glob('../data/interim/youtube/NomadicAmbience_city/*')\n",
    "city_file_list += glob.glob('../data/interim/SONYC/**/*.pkl')\n",
    "city_file_list += glob.glob('../data/interim/S2L_LULC/urban_26_100/*')\n",
    "\n",
    "nature_source_list = ['nature_'+i.rsplit('/', 3)[1]+'/'+i.rsplit('/', 3)[2] for i in nature_file_list]\n",
    "city_source_list = ['city_'+i.rsplit('/', -1)[3] for i in city_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ec37f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/W...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/R...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/A...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/W...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/L...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam111...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam006...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam083...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam052...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam098...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1924 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file  \\\n",
       "0     ../data/interim/AmbisonicSoundLibrary/nature/W...   \n",
       "1     ../data/interim/AmbisonicSoundLibrary/nature/R...   \n",
       "2     ../data/interim/AmbisonicSoundLibrary/nature/A...   \n",
       "3     ../data/interim/AmbisonicSoundLibrary/nature/W...   \n",
       "4     ../data/interim/AmbisonicSoundLibrary/nature/L...   \n",
       "...                                                 ...   \n",
       "1919  ../data/interim/S2L_LULC/urban_26_100/s2lam111...   \n",
       "1920  ../data/interim/S2L_LULC/urban_26_100/s2lam006...   \n",
       "1921  ../data/interim/S2L_LULC/urban_26_100/s2lam083...   \n",
       "1922  ../data/interim/S2L_LULC/urban_26_100/s2lam052...   \n",
       "1923  ../data/interim/S2L_LULC/urban_26_100/s2lam098...   \n",
       "\n",
       "                                   source  category  \n",
       "0     nature_AmbisonicSoundLibrary/nature         0  \n",
       "1     nature_AmbisonicSoundLibrary/nature         0  \n",
       "2     nature_AmbisonicSoundLibrary/nature         0  \n",
       "3     nature_AmbisonicSoundLibrary/nature         0  \n",
       "4     nature_AmbisonicSoundLibrary/nature         0  \n",
       "...                                   ...       ...  \n",
       "1919                        city_S2L_LULC         1  \n",
       "1920                        city_S2L_LULC         1  \n",
       "1921                        city_S2L_LULC         1  \n",
       "1922                        city_S2L_LULC         1  \n",
       "1923                        city_S2L_LULC         1  \n",
       "\n",
       "[1924 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nature_df = pd.DataFrame({'file': nature_file_list, 'source': nature_source_list, 'category': 0})\n",
    "city_df = pd.DataFrame({'file': city_file_list, 'source': city_source_list, 'category': 1})\n",
    "df_all = pd.concat([nature_df, city_df], ignore_index=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26f5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Split the data into folds using StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=23)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_all, df_all['source'])):\n",
    "    # Assign the fold number to each row in the DataFrame\n",
    "    df_all.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "df_all['fold'] = df_all['fold'].astype('int')\n",
    "df_all = df_all.sample(frac=1, random_state=23).reset_index(drop=True) # need to shuffle the rows before deep learning\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d579e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2llg003_1...</td>\n",
       "      <td>nature_S2L_LULC/urban_0_25</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/interim/SONYC/audio-12/34_018803.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>../data/interim/SONYC/audio-4/22_004702.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2lam042_1...</td>\n",
       "      <td>nature_S2L_LULC/urban_0_25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2llg001_1...</td>\n",
       "      <td>nature_S2L_LULC/urban_0_25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>../data/interim/S2L_LULC/non_urban/s2llg004_17...</td>\n",
       "      <td>nature_S2L_LULC/non_urban</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1924 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file  \\\n",
       "0     ../data/interim/S2L_LULC/urban_0_25/s2llg003_1...   \n",
       "1     ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "2          ../data/interim/SONYC/audio-12/34_018803.pkl   \n",
       "3     ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "4     ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "...                                                 ...   \n",
       "1919        ../data/interim/SONYC/audio-4/22_004702.pkl   \n",
       "1920  ../data/interim/S2L_LULC/urban_0_25/s2lam042_1...   \n",
       "1921  ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "1922  ../data/interim/S2L_LULC/urban_0_25/s2llg001_1...   \n",
       "1923  ../data/interim/S2L_LULC/non_urban/s2llg004_17...   \n",
       "\n",
       "                          source  category  fold  \n",
       "0     nature_S2L_LULC/urban_0_25         0     9  \n",
       "1            city_GoogleAudioSet         1     8  \n",
       "2                     city_SONYC         1     7  \n",
       "3            city_GoogleAudioSet         1     4  \n",
       "4            city_GoogleAudioSet         1     0  \n",
       "...                          ...       ...   ...  \n",
       "1919                  city_SONYC         1     1  \n",
       "1920  nature_S2L_LULC/urban_0_25         0     2  \n",
       "1921         city_GoogleAudioSet         1     1  \n",
       "1922  nature_S2L_LULC/urban_0_25         0     0  \n",
       "1923   nature_S2L_LULC/non_urban         0     0  \n",
       "\n",
       "[1924 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.to_csv('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc4905",
   "metadata": {},
   "source": [
    "# Convert data into TF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341a8a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = df_all['file']\n",
    "targets = df_all['category']\n",
    "folds = df_all['fold']\n",
    "\n",
    "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a936059a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short length: 159999\n",
      "short length: 159880\n",
      "short length: 146099\n",
      "short length: 159880\n",
      "short length: 159880\n",
      "short length: 151683\n",
      "short length: 159880\n",
      "short length: 153357\n",
      "short length: 153242\n",
      "short length: 157848\n",
      "short length: 159992\n",
      "short length: 151461\n",
      "short length: 148006\n",
      "short length: 159997\n",
      "short length: 156480\n",
      "short length: 159993\n"
     ]
    }
   ],
   "source": [
    "def load_wav_pkl(filename, wav_label='y'):\n",
    "    import pickle\n",
    "    # open a file, where you stored the pickled data\n",
    "    file = open(filename, 'rb')\n",
    "\n",
    "    # dump information to that file\n",
    "    output = pickle.load(file)\n",
    "    wav = output[wav_label]\n",
    "\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return wav\n",
    "\n",
    "wav_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'])) == 160000:\n",
    "        wav_list.append(load_wav_pkl(row['file']))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'])\n",
    "        print('short length: '+str(len(temp_wav)))\n",
    "        wav_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000\n",
    "\n",
    "wav_bg_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'], 'bg_y')) == 160000:\n",
    "        wav_bg_list.append(load_wav_pkl(row['file'], 'bg_y'))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'], 'bg_y')\n",
    "#         print('short length: '+str(len(temp_wav)))\n",
    "        wav_bg_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000\n",
    "\n",
    "wav_fg_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'], 'fg_y')) == 160000:\n",
    "        wav_fg_list.append(load_wav_pkl(row['file'], 'fg_y'))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'], 'fg_y')\n",
    "#         print('short length: '+str(len(temp_wav)))\n",
    "        wav_fg_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53798c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ds = tf.data.Dataset.from_tensor_slices((np.stack(wav_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_fg = tf.data.Dataset.from_tensor_slices((np.stack(wav_fg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_bg = tf.data.Dataset.from_tensor_slices((np.stack(wav_bg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a63c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding(wav_data, label, fold):\n",
    "    # run YAMNet to extract embedding from the wav data\n",
    "    scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "    num_embeddings = tf.shape(embeddings)[0]\n",
    "    return (embeddings,\n",
    "            tf.repeat(label, num_embeddings),\n",
    "            tf.repeat(fold, num_embeddings))\n",
    "\n",
    "# extract embedding\n",
    "main_ds = main_ds.map(extract_embedding).unbatch()\n",
    "main_ds_fg = main_ds_fg.map(extract_embedding).unbatch()\n",
    "main_ds_bg = main_ds_bg.map(extract_embedding).unbatch()\n",
    "\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a434e4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ds_all3 = tf.data.Dataset.from_tensor_slices((np.stack(wav_list, axis = 0), np.stack(wav_bg_list, axis = 0), np.stack(wav_fg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_all3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "040642bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(3072,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding_3(wav_data_raw, wav_data_bg, wav_data_fg, label, fold):\n",
    "    # run YAMNet to extract embedding from the wav data\n",
    "    scores, embeddings_raw, spectrogram = yamnet_model(wav_data_raw)\n",
    "    scores, embeddings_bg, spectrogram = yamnet_model(wav_data_bg)\n",
    "    scores, embeddings_fg, spectrogram = yamnet_model(wav_data_fg)\n",
    "    num_embeddings_raw = tf.shape(embeddings_raw)[0]\n",
    "    return (tf.concat([embeddings_raw, embeddings_bg, embeddings_fg],1),\n",
    "            tf.repeat(label, num_embeddings_raw),\n",
    "            tf.repeat(fold, num_embeddings_raw))\n",
    "\n",
    "# extract embedding\n",
    "main_ds_3 = main_ds_all3.map(extract_embedding_3).unbatch()\n",
    "\n",
    "main_ds_3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439303be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andrewchang/opt/anaconda3/envs/AcousticEnv/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andrewchang/opt/anaconda3/envs/AcousticEnv/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "## raw signal\n",
    "cached_ds = main_ds.cache()\n",
    "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds = train_ds.map(remove_fold_column)\n",
    "val_ds = val_ds.map(remove_fold_column)\n",
    "test_ds = test_ds.map(remove_fold_column)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f2c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## background sound\n",
    "cached_ds_bg = main_ds_bg.cache()\n",
    "train_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_bg = train_ds_bg.map(remove_fold_column)\n",
    "val_ds_bg = val_ds_bg.map(remove_fold_column)\n",
    "test_ds_bg = test_ds_bg.map(remove_fold_column)\n",
    "\n",
    "train_ds_bg = train_ds_bg.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_bg = val_ds_bg.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_bg = test_ds_bg.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "662d18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## foreground sound\n",
    "cached_ds_fg = main_ds_fg.cache()\n",
    "train_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_fg = train_ds_fg.map(remove_fold_column)\n",
    "val_ds_fg = val_ds_fg.map(remove_fold_column)\n",
    "test_ds_fg = test_ds_fg.map(remove_fold_column)\n",
    "\n",
    "train_ds_fg = train_ds_fg.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_fg = val_ds_fg.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_fg = test_ds_fg.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1911b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all 3 signals\n",
    "cached_ds_3 = main_ds_3.cache()\n",
    "train_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_3 = train_ds_3.map(remove_fold_column)\n",
    "val_ds_3 = val_ds_3.map(remove_fold_column)\n",
    "test_ds_3 = test_ds_3.map(remove_fold_column)\n",
    "\n",
    "train_ds_3 = train_ds_3.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_3 = val_ds_3.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_3 = test_ds_3.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe243be",
   "metadata": {},
   "source": [
    "# Model of raw signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd43949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"raw_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "raw_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='raw_model')\n",
    "\n",
    "raw_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f392d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# callback will be used in the other models below too\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0259ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 166s 166ms/step - loss: 0.4325 - accuracy: 0.8105 - val_loss: 0.4604 - val_accuracy: 0.7922\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.3810 - accuracy: 0.8321 - val_loss: 0.5047 - val_accuracy: 0.7964\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.3396 - accuracy: 0.8526 - val_loss: 0.5078 - val_accuracy: 0.7984\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.3026 - accuracy: 0.8659 - val_loss: 0.5178 - val_accuracy: 0.7854\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.2803 - accuracy: 0.8772 - val_loss: 0.5396 - val_accuracy: 0.7945\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.2570 - accuracy: 0.8872 - val_loss: 0.5657 - val_accuracy: 0.7982\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.2379 - accuracy: 0.8990 - val_loss: 0.5675 - val_accuracy: 0.7922\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.2262 - accuracy: 0.9060 - val_loss: 0.5850 - val_accuracy: 0.7932\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.2089 - accuracy: 0.9145 - val_loss: 0.6138 - val_accuracy: 0.7953\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.1922 - accuracy: 0.9219 - val_loss: 0.6534 - val_accuracy: 0.7812\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.1783 - accuracy: 0.9275 - val_loss: 0.6689 - val_accuracy: 0.7896\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 9s 9ms/step - loss: 0.1637 - accuracy: 0.9332 - val_loss: 0.7197 - val_accuracy: 0.7896\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 7s 8ms/step - loss: 0.1539 - accuracy: 0.9377 - val_loss: 0.7179 - val_accuracy: 0.7917\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.1459 - accuracy: 0.9436 - val_loss: 0.7597 - val_accuracy: 0.7833\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.1359 - accuracy: 0.9481 - val_loss: 0.7927 - val_accuracy: 0.7914\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.1233 - accuracy: 0.9502 - val_loss: 0.8271 - val_accuracy: 0.7932\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 7s 8ms/step - loss: 0.1219 - accuracy: 0.9531 - val_loss: 0.8456 - val_accuracy: 0.7812\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.1107 - accuracy: 0.9578 - val_loss: 0.9139 - val_accuracy: 0.7826\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.1024 - accuracy: 0.9608 - val_loss: 0.9406 - val_accuracy: 0.7917\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 8s 8ms/step - loss: 0.0994 - accuracy: 0.9612 - val_loss: 0.9080 - val_accuracy: 0.7885\n"
     ]
    }
   ],
   "source": [
    "history = raw_model.fit(train_ds,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d05995ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 4s 19ms/step - loss: 1.2211 - accuracy: 0.7695\n",
      "Loss:  1.2211443185806274\n",
      "Accuracy:  0.76953125\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = raw_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240d839",
   "metadata": {},
   "source": [
    "# Model of background signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40f53bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bg_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "bg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='bg_model')\n",
    "\n",
    "bg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acc05654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.4438 - accuracy: 0.8095 - val_loss: 0.4667 - val_accuracy: 0.7924\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.3720 - accuracy: 0.8360 - val_loss: 0.4866 - val_accuracy: 0.7956\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.3431 - accuracy: 0.8555 - val_loss: 0.4732 - val_accuracy: 0.8016\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3107 - accuracy: 0.8714 - val_loss: 0.4866 - val_accuracy: 0.8010\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 7s 8ms/step - loss: 0.2782 - accuracy: 0.8836 - val_loss: 0.5027 - val_accuracy: 0.7995\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.2568 - accuracy: 0.8920 - val_loss: 0.5269 - val_accuracy: 0.8039\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 9s 10ms/step - loss: 0.2358 - accuracy: 0.9011 - val_loss: 0.5424 - val_accuracy: 0.7924\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 12s 13ms/step - loss: 0.2234 - accuracy: 0.9088 - val_loss: 0.5533 - val_accuracy: 0.7956\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 9s 9ms/step - loss: 0.2011 - accuracy: 0.9177 - val_loss: 0.5805 - val_accuracy: 0.7961\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.1823 - accuracy: 0.9243 - val_loss: 0.6166 - val_accuracy: 0.7906\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.1776 - accuracy: 0.9290 - val_loss: 0.6225 - val_accuracy: 0.7883\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 7s 8ms/step - loss: 0.1605 - accuracy: 0.9360 - val_loss: 0.6364 - val_accuracy: 0.7904\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 8s 8ms/step - loss: 0.1529 - accuracy: 0.9415 - val_loss: 0.6654 - val_accuracy: 0.7932\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.1348 - accuracy: 0.9462 - val_loss: 0.7051 - val_accuracy: 0.7901\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.1453 - accuracy: 0.9477 - val_loss: 0.7749 - val_accuracy: 0.7836\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.1204 - accuracy: 0.9542 - val_loss: 0.8082 - val_accuracy: 0.7872\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.1102 - accuracy: 0.9564 - val_loss: 0.8244 - val_accuracy: 0.7898\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.1103 - accuracy: 0.9574 - val_loss: 0.8614 - val_accuracy: 0.7831\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.0996 - accuracy: 0.9617 - val_loss: 0.9368 - val_accuracy: 0.7812\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.0938 - accuracy: 0.9629 - val_loss: 0.9323 - val_accuracy: 0.7919\n"
     ]
    }
   ],
   "source": [
    "bg_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = bg_model.fit(train_ds_bg,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_bg,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "298a1656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 2ms/step - loss: 1.1918 - accuracy: 0.7672\n",
      "Loss:  1.1917799711227417\n",
      "Accuracy:  0.7671874761581421\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = bg_model.evaluate(test_ds_bg)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5dd27",
   "metadata": {},
   "source": [
    "# Model of foreground signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c377256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fg_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "fg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='fg_model')\n",
    "\n",
    "fg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57973000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 168s 169ms/step - loss: 0.6165 - accuracy: 0.6701 - val_loss: 0.6216 - val_accuracy: 0.6576\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.5808 - accuracy: 0.6917 - val_loss: 0.6323 - val_accuracy: 0.6583\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.5682 - accuracy: 0.7026 - val_loss: 0.6215 - val_accuracy: 0.6656\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.5530 - accuracy: 0.7115 - val_loss: 0.6316 - val_accuracy: 0.6680\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.5373 - accuracy: 0.7220 - val_loss: 0.6361 - val_accuracy: 0.6628\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.5204 - accuracy: 0.7342 - val_loss: 0.6408 - val_accuracy: 0.6677\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.5071 - accuracy: 0.7434 - val_loss: 0.6512 - val_accuracy: 0.6716\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 8s 8ms/step - loss: 0.4937 - accuracy: 0.7530 - val_loss: 0.6678 - val_accuracy: 0.6609\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.4774 - accuracy: 0.7644 - val_loss: 0.6828 - val_accuracy: 0.6617\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.4601 - accuracy: 0.7746 - val_loss: 0.6901 - val_accuracy: 0.6656\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.4470 - accuracy: 0.7841 - val_loss: 0.7136 - val_accuracy: 0.6607\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.4310 - accuracy: 0.7902 - val_loss: 0.7239 - val_accuracy: 0.6646\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.4130 - accuracy: 0.8026 - val_loss: 0.7375 - val_accuracy: 0.6570\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.4012 - accuracy: 0.8089 - val_loss: 0.7888 - val_accuracy: 0.6508\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3851 - accuracy: 0.8203 - val_loss: 0.7853 - val_accuracy: 0.6612\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.3692 - accuracy: 0.8288 - val_loss: 0.8186 - val_accuracy: 0.6586\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 5s 6ms/step - loss: 0.3535 - accuracy: 0.8357 - val_loss: 0.8537 - val_accuracy: 0.6609\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3387 - accuracy: 0.8457 - val_loss: 0.8957 - val_accuracy: 0.6523\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3236 - accuracy: 0.8523 - val_loss: 0.9285 - val_accuracy: 0.6531\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3136 - accuracy: 0.8577 - val_loss: 0.9531 - val_accuracy: 0.6503\n"
     ]
    }
   ],
   "source": [
    "fg_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = fg_model.fit(train_ds_fg,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_fg,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fc48ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 8ms/step - loss: 0.9817 - accuracy: 0.6583\n",
      "Loss:  0.9817324876785278\n",
      "Accuracy:  0.6583333611488342\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = fg_model.evaluate(test_ds_fg)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d976321",
   "metadata": {},
   "source": [
    "# Model of 3 signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "226549b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"all3_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1024)              3146752   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,672,578\n",
      "Trainable params: 3,672,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "tf.keras.backend.clear_session()\n",
    "all3_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3072), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='all3_model')\n",
    "\n",
    "all3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5022e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 22s 22ms/step - loss: 0.4376 - accuracy: 0.8081 - val_loss: 0.5267 - val_accuracy: 0.7729\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 21s 22ms/step - loss: 0.3638 - accuracy: 0.8392 - val_loss: 0.5926 - val_accuracy: 0.7893\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 23s 24ms/step - loss: 0.3155 - accuracy: 0.8603 - val_loss: 0.6279 - val_accuracy: 0.7664\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 23s 23ms/step - loss: 0.2736 - accuracy: 0.8820 - val_loss: 0.7581 - val_accuracy: 0.7589\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 24s 25ms/step - loss: 0.2426 - accuracy: 0.8964 - val_loss: 0.7751 - val_accuracy: 0.7734\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 22s 23ms/step - loss: 0.2106 - accuracy: 0.9107 - val_loss: 0.7680 - val_accuracy: 0.7701\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 23s 23ms/step - loss: 0.1872 - accuracy: 0.9219 - val_loss: 0.8642 - val_accuracy: 0.7956\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 22s 23ms/step - loss: 0.1674 - accuracy: 0.9313 - val_loss: 0.9612 - val_accuracy: 0.7589\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 22s 23ms/step - loss: 0.1550 - accuracy: 0.9369 - val_loss: 0.9292 - val_accuracy: 0.7844\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 22s 23ms/step - loss: 0.1339 - accuracy: 0.9439 - val_loss: 1.1316 - val_accuracy: 0.7826\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 22s 23ms/step - loss: 0.1262 - accuracy: 0.9486 - val_loss: 1.1323 - val_accuracy: 0.7807\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 22s 23ms/step - loss: 0.1198 - accuracy: 0.9508 - val_loss: 1.2302 - val_accuracy: 0.7891\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 23s 23ms/step - loss: 0.1125 - accuracy: 0.9548 - val_loss: 1.1679 - val_accuracy: 0.7828\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 23s 24ms/step - loss: 0.0994 - accuracy: 0.9599 - val_loss: 1.4885 - val_accuracy: 0.7773\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 22s 23ms/step - loss: 0.1081 - accuracy: 0.9590 - val_loss: 1.2937 - val_accuracy: 0.7841\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 23s 24ms/step - loss: 0.0960 - accuracy: 0.9621 - val_loss: 1.4171 - val_accuracy: 0.7888\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 23s 24ms/step - loss: 0.0844 - accuracy: 0.9656 - val_loss: 1.5416 - val_accuracy: 0.7708\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 23s 24ms/step - loss: 0.0872 - accuracy: 0.9661 - val_loss: 1.4172 - val_accuracy: 0.7948\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 24s 25ms/step - loss: 0.0869 - accuracy: 0.9658 - val_loss: 1.4990 - val_accuracy: 0.7818\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 24s 25ms/step - loss: 0.0874 - accuracy: 0.9660 - val_loss: 1.6452 - val_accuracy: 0.7771\n"
     ]
    }
   ],
   "source": [
    "all3_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# callback will be used in the other models below too\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "\n",
    "history = all3_model.fit(train_ds_3,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_3,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a62b1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 1s 4ms/step - loss: 1.8276 - accuracy: 0.7534\n",
      "Loss:  1.827581524848938\n",
      "Accuracy:  0.7533854246139526\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = all3_model.evaluate(test_ds_3)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8b29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
