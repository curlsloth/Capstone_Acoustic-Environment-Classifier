{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e748249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 00:17:16.856233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 00:17:20.506417: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f92587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan data directories\n",
    "import glob\n",
    "\n",
    "nature_file_list = []\n",
    "nature_file_list += glob.glob('../data/interim/AmbisonicSoundLibrary/nature/*')\n",
    "nature_file_list += glob.glob('../data/interim/GoogleAudioSet/Outside, rural or natural/*')\n",
    "nature_file_list += glob.glob('../data/interim/youtube/NatureSoundscapes/*')\n",
    "nature_file_list += glob.glob('../data/interim/youtube/NomadicAmbience_nature/*')\n",
    "nature_file_list += glob.glob('../data/interim/S2L_LULC/non_urban/*')\n",
    "nature_file_list += glob.glob('../data/interim/S2L_LULC/urban_0_25/*')\n",
    "\n",
    "city_file_list = []\n",
    "city_file_list += glob.glob('../data/interim/GoogleAudioSet/Outside, urban or manmade/*')\n",
    "city_file_list += glob.glob('../data/interim/youtube/NomadicAmbience_city/*')\n",
    "city_file_list += glob.glob('../data/interim/SONYC/**/*.pkl')\n",
    "city_file_list += glob.glob('../data/interim/S2L_LULC/urban_26_100/*')\n",
    "\n",
    "nature_source_list = ['nature_'+i.rsplit('/', 3)[1]+'/'+i.rsplit('/', 3)[2] for i in nature_file_list]\n",
    "city_source_list = ['city_'+i.rsplit('/', -1)[3] for i in city_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ec37f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/W...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/R...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/A...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/W...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/L...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam111...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam006...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam083...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam052...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam098...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1924 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file  \\\n",
       "0     ../data/interim/AmbisonicSoundLibrary/nature/W...   \n",
       "1     ../data/interim/AmbisonicSoundLibrary/nature/R...   \n",
       "2     ../data/interim/AmbisonicSoundLibrary/nature/A...   \n",
       "3     ../data/interim/AmbisonicSoundLibrary/nature/W...   \n",
       "4     ../data/interim/AmbisonicSoundLibrary/nature/L...   \n",
       "...                                                 ...   \n",
       "1919  ../data/interim/S2L_LULC/urban_26_100/s2lam111...   \n",
       "1920  ../data/interim/S2L_LULC/urban_26_100/s2lam006...   \n",
       "1921  ../data/interim/S2L_LULC/urban_26_100/s2lam083...   \n",
       "1922  ../data/interim/S2L_LULC/urban_26_100/s2lam052...   \n",
       "1923  ../data/interim/S2L_LULC/urban_26_100/s2lam098...   \n",
       "\n",
       "                                   source  category  \n",
       "0     nature_AmbisonicSoundLibrary/nature         0  \n",
       "1     nature_AmbisonicSoundLibrary/nature         0  \n",
       "2     nature_AmbisonicSoundLibrary/nature         0  \n",
       "3     nature_AmbisonicSoundLibrary/nature         0  \n",
       "4     nature_AmbisonicSoundLibrary/nature         0  \n",
       "...                                   ...       ...  \n",
       "1919                        city_S2L_LULC         1  \n",
       "1920                        city_S2L_LULC         1  \n",
       "1921                        city_S2L_LULC         1  \n",
       "1922                        city_S2L_LULC         1  \n",
       "1923                        city_S2L_LULC         1  \n",
       "\n",
       "[1924 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nature_df = pd.DataFrame({'file': nature_file_list, 'source': nature_source_list, 'category': 0})\n",
    "city_df = pd.DataFrame({'file': city_file_list, 'source': city_source_list, 'category': 1})\n",
    "df_all = pd.concat([nature_df, city_df], ignore_index=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26f5115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2llg003_1...</td>\n",
       "      <td>nature_S2L_LULC/urban_0_25</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/interim/SONYC/audio-12/34_018803.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>../data/interim/SONYC/audio-4/22_004702.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2lam042_1...</td>\n",
       "      <td>nature_S2L_LULC/urban_0_25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2llg001_1...</td>\n",
       "      <td>nature_S2L_LULC/urban_0_25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>../data/interim/S2L_LULC/non_urban/s2llg004_17...</td>\n",
       "      <td>nature_S2L_LULC/non_urban</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1924 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file  \\\n",
       "0     ../data/interim/S2L_LULC/urban_0_25/s2llg003_1...   \n",
       "1     ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "2          ../data/interim/SONYC/audio-12/34_018803.pkl   \n",
       "3     ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "4     ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "...                                                 ...   \n",
       "1919        ../data/interim/SONYC/audio-4/22_004702.pkl   \n",
       "1920  ../data/interim/S2L_LULC/urban_0_25/s2lam042_1...   \n",
       "1921  ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "1922  ../data/interim/S2L_LULC/urban_0_25/s2llg001_1...   \n",
       "1923  ../data/interim/S2L_LULC/non_urban/s2llg004_17...   \n",
       "\n",
       "                          source  category  fold  \n",
       "0     nature_S2L_LULC/urban_0_25         0     9  \n",
       "1            city_GoogleAudioSet         1     8  \n",
       "2                     city_SONYC         1     7  \n",
       "3            city_GoogleAudioSet         1     4  \n",
       "4            city_GoogleAudioSet         1     0  \n",
       "...                          ...       ...   ...  \n",
       "1919                  city_SONYC         1     1  \n",
       "1920  nature_S2L_LULC/urban_0_25         0     2  \n",
       "1921         city_GoogleAudioSet         1     1  \n",
       "1922  nature_S2L_LULC/urban_0_25         0     0  \n",
       "1923   nature_S2L_LULC/non_urban         0     0  \n",
       "\n",
       "[1924 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Split the data into folds using StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=23)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_all, df_all['source'])):\n",
    "    # Assign the fold number to each row in the DataFrame\n",
    "    df_all.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "df_all['fold'] = df_all['fold'].astype('int')\n",
    "df_all = df_all.sample(frac=1, random_state=23).reset_index(drop=True) # need to shuffle the rows before deep learning\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d579e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('../data/interim/train_val_test_split_Feb21.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc4905",
   "metadata": {},
   "source": [
    "# Convert data into TF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341a8a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = df_all['file']\n",
    "targets = df_all['category']\n",
    "folds = df_all['fold']\n",
    "\n",
    "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a936059a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short length: 159999\n",
      "short length: 159880\n",
      "short length: 146099\n",
      "short length: 159880\n",
      "short length: 159880\n",
      "short length: 151683\n",
      "short length: 159880\n",
      "short length: 153357\n",
      "short length: 153242\n",
      "short length: 157848\n",
      "short length: 159992\n",
      "short length: 151461\n",
      "short length: 148006\n",
      "short length: 159997\n",
      "short length: 156480\n",
      "short length: 159993\n"
     ]
    }
   ],
   "source": [
    "def load_wav_pkl(filename, wav_label='y'):\n",
    "    import pickle\n",
    "    # open a file, where you stored the pickled data\n",
    "    file = open(filename, 'rb')\n",
    "\n",
    "    # dump information to that file\n",
    "    output = pickle.load(file)\n",
    "    wav = output[wav_label]\n",
    "\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return wav\n",
    "\n",
    "wav_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'])) == 160000:\n",
    "        wav_list.append(load_wav_pkl(row['file']))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'])\n",
    "        print('short length: '+str(len(temp_wav)))\n",
    "        wav_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000\n",
    "\n",
    "wav_bg_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'], 'bg_y')) == 160000:\n",
    "        wav_bg_list.append(load_wav_pkl(row['file'], 'bg_y'))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'], 'bg_y')\n",
    "#         print('short length: '+str(len(temp_wav)))\n",
    "        wav_bg_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000\n",
    "\n",
    "wav_fg_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'], 'fg_y')) == 160000:\n",
    "        wav_fg_list.append(load_wav_pkl(row['file'], 'fg_y'))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'], 'fg_y')\n",
    "#         print('short length: '+str(len(temp_wav)))\n",
    "        wav_fg_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53798c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ds = tf.data.Dataset.from_tensor_slices((np.stack(wav_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_fg = tf.data.Dataset.from_tensor_slices((np.stack(wav_fg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_bg = tf.data.Dataset.from_tensor_slices((np.stack(wav_bg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a63c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding(wav_data, label, fold):\n",
    "    # run YAMNet to extract embedding from the wav data\n",
    "    scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "    num_embeddings = tf.shape(embeddings)[0]\n",
    "    return (embeddings,\n",
    "            tf.repeat(label, num_embeddings),\n",
    "            tf.repeat(fold, num_embeddings))\n",
    "\n",
    "# extract embedding\n",
    "main_ds = main_ds.map(extract_embedding).unbatch()\n",
    "main_ds_fg = main_ds_fg.map(extract_embedding).unbatch()\n",
    "main_ds_bg = main_ds_bg.map(extract_embedding).unbatch()\n",
    "\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "466f3c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ds_all3 = tf.data.Dataset.from_tensor_slices((np.stack(wav_list, axis = 0), np.stack(wav_bg_list, axis = 0), np.stack(wav_fg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_all3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23104af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(3072,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding_3(wav_data_raw, wav_data_bg, wav_data_fg, label, fold):\n",
    "    # run YAMNet to extract embedding from the wav data\n",
    "    scores, embeddings_raw, spectrogram = yamnet_model(wav_data_raw)\n",
    "    scores, embeddings_bg, spectrogram = yamnet_model(wav_data_bg)\n",
    "    scores, embeddings_fg, spectrogram = yamnet_model(wav_data_fg)\n",
    "    num_embeddings_raw = tf.shape(embeddings_raw)[0]\n",
    "    return (tf.concat([embeddings_raw, embeddings_bg, embeddings_fg],1),\n",
    "            tf.repeat(label, num_embeddings_raw),\n",
    "            tf.repeat(fold, num_embeddings_raw))\n",
    "\n",
    "# extract embedding\n",
    "main_ds_3 = main_ds_all3.map(extract_embedding_3).unbatch()\n",
    "\n",
    "main_ds_3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439303be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andrewchang/opt/anaconda3/envs/AcousticEnv/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andrewchang/opt/anaconda3/envs/AcousticEnv/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "## raw signal\n",
    "cached_ds = main_ds.cache()\n",
    "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds = train_ds.map(remove_fold_column)\n",
    "val_ds = val_ds.map(remove_fold_column)\n",
    "test_ds = test_ds.map(remove_fold_column)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f2c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## background sound\n",
    "cached_ds_bg = main_ds_bg.cache()\n",
    "train_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_bg = train_ds_bg.map(remove_fold_column)\n",
    "val_ds_bg = val_ds_bg.map(remove_fold_column)\n",
    "test_ds_bg = test_ds_bg.map(remove_fold_column)\n",
    "\n",
    "train_ds_bg = train_ds_bg.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_bg = val_ds_bg.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_bg = test_ds_bg.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "662d18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## foreground sound\n",
    "cached_ds_fg = main_ds_fg.cache()\n",
    "train_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_fg = train_ds_fg.map(remove_fold_column)\n",
    "val_ds_fg = val_ds_fg.map(remove_fold_column)\n",
    "test_ds_fg = test_ds_fg.map(remove_fold_column)\n",
    "\n",
    "train_ds_fg = train_ds_fg.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_fg = val_ds_fg.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_fg = test_ds_fg.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89879ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all 3 signals\n",
    "cached_ds_3 = main_ds_3.cache()\n",
    "train_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_3 = train_ds_3.map(remove_fold_column)\n",
    "val_ds_3 = val_ds_3.map(remove_fold_column)\n",
    "test_ds_3 = test_ds_3.map(remove_fold_column)\n",
    "\n",
    "train_ds_3 = train_ds_3.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_3 = val_ds_3.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_3 = test_ds_3.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe243be",
   "metadata": {},
   "source": [
    "# Model of raw signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd43949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"raw_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(23)\n",
    "\n",
    "raw_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='raw_model')\n",
    "\n",
    "raw_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f392d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# callback will be used in the other models below too\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0259ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 146s 145ms/step - loss: 0.4141 - accuracy: 0.8130 - val_loss: 0.4674 - val_accuracy: 0.7901\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 5s 6ms/step - loss: 0.3703 - accuracy: 0.8350 - val_loss: 0.4708 - val_accuracy: 0.8031\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3561 - accuracy: 0.8569 - val_loss: 0.5595 - val_accuracy: 0.7951\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 7s 8ms/step - loss: 0.3034 - accuracy: 0.8696 - val_loss: 0.5269 - val_accuracy: 0.8005\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 9s 9ms/step - loss: 0.2756 - accuracy: 0.8832 - val_loss: 0.5494 - val_accuracy: 0.8026\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.2627 - accuracy: 0.8916 - val_loss: 0.5327 - val_accuracy: 0.7974\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.2557 - accuracy: 0.9002 - val_loss: 0.5741 - val_accuracy: 0.8008\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.2322 - accuracy: 0.9087 - val_loss: 0.6189 - val_accuracy: 0.8026\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 5s 6ms/step - loss: 0.2177 - accuracy: 0.9175 - val_loss: 0.5920 - val_accuracy: 0.7953\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 4s 5ms/step - loss: 0.1986 - accuracy: 0.9238 - val_loss: 0.6664 - val_accuracy: 0.7911\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.1803 - accuracy: 0.9300 - val_loss: 0.6638 - val_accuracy: 0.7984\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 9s 9ms/step - loss: 0.1645 - accuracy: 0.9347 - val_loss: 0.7629 - val_accuracy: 0.7927\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 10s 10ms/step - loss: 0.1559 - accuracy: 0.9394 - val_loss: 0.7303 - val_accuracy: 0.7974\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 9s 9ms/step - loss: 0.1494 - accuracy: 0.9410 - val_loss: 0.7486 - val_accuracy: 0.7865\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 9s 9ms/step - loss: 0.1359 - accuracy: 0.9461 - val_loss: 0.8296 - val_accuracy: 0.7823\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 8s 9ms/step - loss: 0.1293 - accuracy: 0.9494 - val_loss: 0.8683 - val_accuracy: 0.7828\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 10s 10ms/step - loss: 0.1178 - accuracy: 0.9539 - val_loss: 0.9174 - val_accuracy: 0.7914\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.1155 - accuracy: 0.9552 - val_loss: 0.8769 - val_accuracy: 0.7937\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.1053 - accuracy: 0.9595 - val_loss: 1.0016 - val_accuracy: 0.7880\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.1020 - accuracy: 0.9610 - val_loss: 0.9786 - val_accuracy: 0.7844\n"
     ]
    }
   ],
   "source": [
    "history = raw_model.fit(train_ds,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d05995ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 8ms/step - loss: 1.1933 - accuracy: 0.7617\n",
      "Loss:  1.193291187286377\n",
      "Accuracy:  0.76171875\n"
     ]
    }
   ],
   "source": [
    "# this is the accuracy per YAMNet sample, not the accuracy per audio file\n",
    "loss, accuracy = raw_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ec330d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_by_audio(model, test_ds, y_true):\n",
    "    n_sample_per_audio = 20\n",
    "    y_pred = model.predict(test_ds)\n",
    "    y_reshaped = y_pred.reshape(-1, n_sample_per_audio, 2) # reshape to audio*sample*binary_prediction\n",
    "    y_pred_by_row = y_reshaped.mean(axis=1).argmax(axis=1)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(y_true, y_pred_by_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db50fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 2ms/step\n",
      "0.7916666666666666\n"
     ]
    }
   ],
   "source": [
    "test_df = df_all[df_all['fold']==9]\n",
    "acc = acc_by_audio(raw_model, test_ds, y_true = test_df['category'])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240d839",
   "metadata": {},
   "source": [
    "# Model of background signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40f53bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bg_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(23)\n",
    "\n",
    "bg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='bg_model')\n",
    "\n",
    "bg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acc05654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 168s 168ms/step - loss: 0.4350 - accuracy: 0.8094 - val_loss: 0.4543 - val_accuracy: 0.7943\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 10s 10ms/step - loss: 0.4080 - accuracy: 0.8378 - val_loss: 0.4682 - val_accuracy: 0.7945\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 11s 12ms/step - loss: 0.3684 - accuracy: 0.8526 - val_loss: 0.4979 - val_accuracy: 0.7961\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 9s 9ms/step - loss: 0.3104 - accuracy: 0.8675 - val_loss: 0.5076 - val_accuracy: 0.7943\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 8s 8ms/step - loss: 0.3031 - accuracy: 0.8797 - val_loss: 0.5115 - val_accuracy: 0.7906\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.2805 - accuracy: 0.8900 - val_loss: 0.5381 - val_accuracy: 0.7979\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.2414 - accuracy: 0.8991 - val_loss: 0.5447 - val_accuracy: 0.7831\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.2235 - accuracy: 0.9072 - val_loss: 0.5746 - val_accuracy: 0.7872\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 7s 8ms/step - loss: 0.2137 - accuracy: 0.9160 - val_loss: 0.5716 - val_accuracy: 0.7888\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 8s 8ms/step - loss: 0.1926 - accuracy: 0.9210 - val_loss: 0.6144 - val_accuracy: 0.7951\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 8s 9ms/step - loss: 0.1811 - accuracy: 0.9269 - val_loss: 0.6475 - val_accuracy: 0.7901\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.1682 - accuracy: 0.9333 - val_loss: 0.6901 - val_accuracy: 0.7797\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 9s 9ms/step - loss: 0.1611 - accuracy: 0.9373 - val_loss: 0.6764 - val_accuracy: 0.7906\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.1467 - accuracy: 0.9429 - val_loss: 0.7319 - val_accuracy: 0.7919\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 9s 9ms/step - loss: 0.1401 - accuracy: 0.9447 - val_loss: 0.7600 - val_accuracy: 0.7896\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 8s 8ms/step - loss: 0.1263 - accuracy: 0.9506 - val_loss: 0.7777 - val_accuracy: 0.7844\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 8s 8ms/step - loss: 0.1159 - accuracy: 0.9555 - val_loss: 0.8280 - val_accuracy: 0.7839\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 9s 10ms/step - loss: 0.1140 - accuracy: 0.9555 - val_loss: 0.8606 - val_accuracy: 0.7888\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 12s 13ms/step - loss: 0.1071 - accuracy: 0.9584 - val_loss: 0.9090 - val_accuracy: 0.7831\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 11s 12ms/step - loss: 0.0997 - accuracy: 0.9614 - val_loss: 0.9110 - val_accuracy: 0.7875\n"
     ]
    }
   ],
   "source": [
    "bg_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = bg_model.fit(train_ds_bg,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_bg,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "298a1656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 9ms/step - loss: 1.1454 - accuracy: 0.7661\n",
      "Loss:  1.1454471349716187\n",
      "Accuracy:  0.7661458253860474\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = bg_model.evaluate(test_ds_bg)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "065702ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 2ms/step\n",
      "0.8229166666666666\n"
     ]
    }
   ],
   "source": [
    "test_df = df_all[df_all['fold']==9]\n",
    "acc = acc_by_audio(bg_model, test_ds_bg, y_true = test_df['category'])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5dd27",
   "metadata": {},
   "source": [
    "# Model of foreground signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c377256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fg_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(23)\n",
    "\n",
    "fg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='fg_model')\n",
    "\n",
    "fg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57973000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 176s 174ms/step - loss: 0.6339 - accuracy: 0.6659 - val_loss: 0.6146 - val_accuracy: 0.6576\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.5807 - accuracy: 0.6928 - val_loss: 0.6320 - val_accuracy: 0.6549\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.5663 - accuracy: 0.7011 - val_loss: 0.6302 - val_accuracy: 0.6552\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.5572 - accuracy: 0.7059 - val_loss: 0.6300 - val_accuracy: 0.6661\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.5395 - accuracy: 0.7195 - val_loss: 0.6279 - val_accuracy: 0.6708\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 7s 8ms/step - loss: 0.5278 - accuracy: 0.7303 - val_loss: 0.6316 - val_accuracy: 0.6643\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.5119 - accuracy: 0.7403 - val_loss: 0.6474 - val_accuracy: 0.6609\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.4991 - accuracy: 0.7501 - val_loss: 0.6595 - val_accuracy: 0.6648\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.4805 - accuracy: 0.7588 - val_loss: 0.6779 - val_accuracy: 0.6721\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.4661 - accuracy: 0.7713 - val_loss: 0.6833 - val_accuracy: 0.6568\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.4514 - accuracy: 0.7800 - val_loss: 0.6914 - val_accuracy: 0.6635\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.4332 - accuracy: 0.7906 - val_loss: 0.7082 - val_accuracy: 0.6620\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.4155 - accuracy: 0.8019 - val_loss: 0.7447 - val_accuracy: 0.6643\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.4004 - accuracy: 0.8111 - val_loss: 0.7607 - val_accuracy: 0.6589\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.3833 - accuracy: 0.8185 - val_loss: 0.7935 - val_accuracy: 0.6536\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.3704 - accuracy: 0.8263 - val_loss: 0.8433 - val_accuracy: 0.6474\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 7s 8ms/step - loss: 0.3547 - accuracy: 0.8364 - val_loss: 0.8498 - val_accuracy: 0.6479\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.3413 - accuracy: 0.8417 - val_loss: 0.8900 - val_accuracy: 0.6542\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3265 - accuracy: 0.8507 - val_loss: 0.9357 - val_accuracy: 0.6562\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3146 - accuracy: 0.8572 - val_loss: 0.9477 - val_accuracy: 0.6445\n"
     ]
    }
   ],
   "source": [
    "fg_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = fg_model.fit(train_ds_fg,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_fg,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fc48ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 8ms/step - loss: 0.9528 - accuracy: 0.6534\n",
      "Loss:  0.9527502059936523\n",
      "Accuracy:  0.6533854007720947\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = fg_model.evaluate(test_ds_fg)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38a7902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 2ms/step\n",
      "0.7604166666666666\n"
     ]
    }
   ],
   "source": [
    "test_df = df_all[df_all['fold']==9]\n",
    "acc = acc_by_audio(fg_model, test_ds_fg, y_true = test_df['category'])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f696c",
   "metadata": {},
   "source": [
    "# Model of 3 signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2e3e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"all3_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1024)              3146752   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,672,578\n",
      "Trainable params: 3,672,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(23)\n",
    "\n",
    "all3_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3072), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='all3_model')\n",
    "\n",
    "all3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f113020",
   "metadata": {},
   "outputs": [],
   "source": [
    "all3_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# callback will be used in the other models below too\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "\n",
    "history = all3_model.fit(train_ds_3,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_3,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = all3_model.evaluate(test_ds_3)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_all[df_all['fold']==9]\n",
    "acc = acc_by_audio(all3_model, test_ds_3, y_true = test_df['category'])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0db323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
