{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e748249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 18:09:42.058637: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 18:09:46.009107: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle) # if there's an \"SavedModel file does not exist at:\", delete that folder and rerun it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f92587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan data directories\n",
    "import glob\n",
    "\n",
    "nature_file_list = []\n",
    "nature_file_list += glob.glob('../data/interim/AmbisonicSoundLibrary/nature/*')\n",
    "nature_file_list += glob.glob('../data/interim/GoogleAudioSet/Outside, rural or natural/*')\n",
    "nature_file_list += glob.glob('../data/interim/youtube/NomadicAmbience_nature/*')\n",
    "nature_file_list += glob.glob('../data/interim/S2L_LULC/non_urban/*')\n",
    "nature_file_list += glob.glob('../data/interim/S2L_LULC/urban_0_25/*')\n",
    "\n",
    "city_file_list = []\n",
    "city_file_list += glob.glob('../data/interim/GoogleAudioSet/Outside, urban or manmade/*')\n",
    "city_file_list += glob.glob('../data/interim/youtube/NomadicAmbience_city/*')\n",
    "# city_file_list += glob.glob('../data/interim/S2L_LULC/urban_26_100/*')\n",
    "\n",
    "# these two directories have way too many files, which will biase the training toward the difference between these 2 them rather the category\n",
    "NS_file_list = glob.glob('../data/interim/youtube/NatureSoundscapes/*')\n",
    "SONYC_file_list = glob.glob('../data/interim/SONYC/**/*.pkl')\n",
    "\n",
    "import random\n",
    "random.seed(23)\n",
    "n_files_minus = 1600\n",
    "NS_file_list_sampled = random.sample(NS_file_list, len(NS_file_list)-n_files_minus)\n",
    "SONYC_file_list_sampled = random.sample(SONYC_file_list, len(SONYC_file_list)-n_files_minus)\n",
    "\n",
    "nature_file_list += NS_file_list_sampled\n",
    "city_file_list += SONYC_file_list_sampled \n",
    "\n",
    "nature_source_list = ['nature_'+i.rsplit('/', 3)[1]+'/'+i.rsplit('/', 3)[2] for i in nature_file_list]\n",
    "city_source_list = ['city_'+i.rsplit('/', -1)[3] for i in city_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ec37f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "city_GoogleAudioSet                                170\n",
       "city_SONYC                                         709\n",
       "city_youtube                                       244\n",
       "nature_AmbisonicSoundLibrary/nature                 58\n",
       "nature_GoogleAudioSet/Outside, rural or natural    160\n",
       "nature_S2L_LULC/non_urban                          240\n",
       "nature_S2L_LULC/urban_0_25                         240\n",
       "nature_youtube/NatureSoundscapes                   486\n",
       "nature_youtube/NomadicAmbience_nature              175\n",
       "Name: file, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nature_df = pd.DataFrame({'file': nature_file_list, 'source': nature_source_list, 'category': 0})\n",
    "city_df = pd.DataFrame({'file': city_file_list, 'source': city_source_list, 'category': 1})\n",
    "df_all = pd.concat([nature_df, city_df], ignore_index=True)\n",
    "df_all.groupby(['source'])['file'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ee75b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>file_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/W...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/R...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/A...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/W...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/L...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>../data/interim/SONYC/audio-6/32_004222.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>../data/interim/SONYC/audio-6/32_004222.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>../data/interim/SONYC/audio-17/35_011631.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>../data/interim/SONYC/audio-17/35_011631.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>../data/interim/SONYC/audio-18/06_001066.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>../data/interim/SONYC/audio-18/06_001066.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>../data/interim/SONYC/audio-14/41_019620.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>../data/interim/SONYC/audio-14/41_019620.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>../data/interim/SONYC/audio-11/06_019853.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>../data/interim/SONYC/audio-11/06_019853.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2482 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file  \\\n",
       "0     ../data/interim/AmbisonicSoundLibrary/nature/W...   \n",
       "1     ../data/interim/AmbisonicSoundLibrary/nature/R...   \n",
       "2     ../data/interim/AmbisonicSoundLibrary/nature/A...   \n",
       "3     ../data/interim/AmbisonicSoundLibrary/nature/W...   \n",
       "4     ../data/interim/AmbisonicSoundLibrary/nature/L...   \n",
       "...                                                 ...   \n",
       "2477        ../data/interim/SONYC/audio-6/32_004222.pkl   \n",
       "2478       ../data/interim/SONYC/audio-17/35_011631.pkl   \n",
       "2479       ../data/interim/SONYC/audio-18/06_001066.pkl   \n",
       "2480       ../data/interim/SONYC/audio-14/41_019620.pkl   \n",
       "2481       ../data/interim/SONYC/audio-11/06_019853.pkl   \n",
       "\n",
       "                                   source  category  \\\n",
       "0     nature_AmbisonicSoundLibrary/nature         0   \n",
       "1     nature_AmbisonicSoundLibrary/nature         0   \n",
       "2     nature_AmbisonicSoundLibrary/nature         0   \n",
       "3     nature_AmbisonicSoundLibrary/nature         0   \n",
       "4     nature_AmbisonicSoundLibrary/nature         0   \n",
       "...                                   ...       ...   \n",
       "2477                           city_SONYC         1   \n",
       "2478                           city_SONYC         1   \n",
       "2479                           city_SONYC         1   \n",
       "2480                           city_SONYC         1   \n",
       "2481                           city_SONYC         1   \n",
       "\n",
       "                                             file_group  \n",
       "0     ../data/interim/AmbisonicSoundLibrary/nature/W...  \n",
       "1     ../data/interim/AmbisonicSoundLibrary/nature/R...  \n",
       "2     ../data/interim/AmbisonicSoundLibrary/nature/A...  \n",
       "3     ../data/interim/AmbisonicSoundLibrary/nature/W...  \n",
       "4     ../data/interim/AmbisonicSoundLibrary/nature/L...  \n",
       "...                                                 ...  \n",
       "2477        ../data/interim/SONYC/audio-6/32_004222.pkl  \n",
       "2478       ../data/interim/SONYC/audio-17/35_011631.pkl  \n",
       "2479       ../data/interim/SONYC/audio-18/06_001066.pkl  \n",
       "2480       ../data/interim/SONYC/audio-14/41_019620.pkl  \n",
       "2481       ../data/interim/SONYC/audio-11/06_019853.pkl  \n",
       "\n",
       "[2482 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the youtube ID of the file out and put it as 'file_group'\n",
    "df_all['file_group'] = df_all['file'].apply(lambda st: st[st.find('/youtube/')+1:st.rfind(\"_\")])\n",
    "\n",
    "# for the rows that were not extracted from youtube, replace the 'file_group' by 'file', as they are all from different sources\n",
    "df_all.loc[~df_all['file_group'].str.contains('youtube'), 'file_group'] = df_all.loc[~df_all['file_group'].str.contains('youtube'), 'file']\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ee6fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>file_group</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/interim/S2L_LULC/non_urban/s2lam112_21...</td>\n",
       "      <td>nature_S2L_LULC/non_urban</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/interim/S2L_LULC/non_urban/s2lam112_21...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/interim/youtube/NomadicAmbience_city/-...</td>\n",
       "      <td>city_youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>youtube/NomadicAmbience_city/-veiV1hHEbA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/interim/youtube/NomadicAmbience_nature...</td>\n",
       "      <td>nature_youtube/NomadicAmbience_nature</td>\n",
       "      <td>0</td>\n",
       "      <td>youtube/NomadicAmbience_nature/On_o9K0PBAs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/interim/youtube/NomadicAmbience_city/Y...</td>\n",
       "      <td>city_youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>youtube/NomadicAmbience_city/YNAdzvW-ZTc</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/interim/youtube/NomadicAmbience_city/7...</td>\n",
       "      <td>city_youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>youtube/NomadicAmbience_city/7dfpryACUV4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>../data/interim/SONYC/audio-10/41_008799.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>../data/interim/SONYC/audio-10/41_008799.pkl</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>../data/interim/youtube/NatureSoundscapes/fpPF...</td>\n",
       "      <td>nature_youtube/NatureSoundscapes</td>\n",
       "      <td>0</td>\n",
       "      <td>youtube/NatureSoundscapes/fpPF-uoLaCE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2lam051_2...</td>\n",
       "      <td>nature_S2L_LULC/urban_0_25</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2lam051_2...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>../data/interim/S2L_LULC/non_urban/s2lam107_21...</td>\n",
       "      <td>nature_S2L_LULC/non_urban</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/interim/S2L_LULC/non_urban/s2lam107_21...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2482 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file  \\\n",
       "0     ../data/interim/S2L_LULC/non_urban/s2lam112_21...   \n",
       "1     ../data/interim/youtube/NomadicAmbience_city/-...   \n",
       "2     ../data/interim/youtube/NomadicAmbience_nature...   \n",
       "3     ../data/interim/youtube/NomadicAmbience_city/Y...   \n",
       "4     ../data/interim/youtube/NomadicAmbience_city/7...   \n",
       "...                                                 ...   \n",
       "2477  ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "2478       ../data/interim/SONYC/audio-10/41_008799.pkl   \n",
       "2479  ../data/interim/youtube/NatureSoundscapes/fpPF...   \n",
       "2480  ../data/interim/S2L_LULC/urban_0_25/s2lam051_2...   \n",
       "2481  ../data/interim/S2L_LULC/non_urban/s2lam107_21...   \n",
       "\n",
       "                                     source  category  \\\n",
       "0                 nature_S2L_LULC/non_urban         0   \n",
       "1                              city_youtube         1   \n",
       "2     nature_youtube/NomadicAmbience_nature         0   \n",
       "3                              city_youtube         1   \n",
       "4                              city_youtube         1   \n",
       "...                                     ...       ...   \n",
       "2477                    city_GoogleAudioSet         1   \n",
       "2478                             city_SONYC         1   \n",
       "2479       nature_youtube/NatureSoundscapes         0   \n",
       "2480             nature_S2L_LULC/urban_0_25         0   \n",
       "2481              nature_S2L_LULC/non_urban         0   \n",
       "\n",
       "                                             file_group  fold  \n",
       "0     ../data/interim/S2L_LULC/non_urban/s2lam112_21...     0  \n",
       "1              youtube/NomadicAmbience_city/-veiV1hHEbA     1  \n",
       "2            youtube/NomadicAmbience_nature/On_o9K0PBAs     1  \n",
       "3              youtube/NomadicAmbience_city/YNAdzvW-ZTc     4  \n",
       "4              youtube/NomadicAmbience_city/7dfpryACUV4     7  \n",
       "...                                                 ...   ...  \n",
       "2477  ../data/interim/GoogleAudioSet/Outside, urban ...     9  \n",
       "2478       ../data/interim/SONYC/audio-10/41_008799.pkl     6  \n",
       "2479              youtube/NatureSoundscapes/fpPF-uoLaCE     1  \n",
       "2480  ../data/interim/S2L_LULC/urban_0_25/s2lam051_2...     6  \n",
       "2481  ../data/interim/S2L_LULC/non_urban/s2lam107_21...     4  \n",
       "\n",
       "[2482 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# Split the data into folds using StratifiedKFold\n",
    "sgkf = StratifiedGroupKFold(n_splits=10, shuffle=True, random_state=23)\n",
    "for fold, (train_idx, val_idx) in enumerate(sgkf.split(X=df_all, y=df_all['source'], groups=df_all['file_group'])):\n",
    "    # Assign the fold number to each row in the DataFrame\n",
    "    df_all.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "df_all['fold'] = df_all['fold'].astype('int')\n",
    "df_all = df_all.sample(frac=1, random_state=23).reset_index(drop=True) # need to shuffle the rows before deep learning\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d579e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.to_csv('../data/interim/train_val_test_split_Feb21.csv')\n",
    "df_all.to_csv('../data/interim/train_val_test_sgkf_Feb22.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc4905",
   "metadata": {},
   "source": [
    "# Convert data into TF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341a8a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = df_all['file']\n",
    "targets = df_all['category']\n",
    "folds = df_all['fold']\n",
    "\n",
    "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a936059a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short length: 151683\n",
      "short length: 159992\n",
      "short length: 156480\n",
      "short length: 146099\n",
      "short length: 153357\n",
      "short length: 159999\n",
      "short length: 159880\n",
      "short length: 148006\n",
      "short length: 153242\n",
      "short length: 159997\n",
      "short length: 159880\n",
      "short length: 159880\n",
      "short length: 159993\n",
      "short length: 159880\n",
      "short length: 157848\n",
      "short length: 151461\n"
     ]
    }
   ],
   "source": [
    "def load_wav_pkl(filename, wav_label='y'):\n",
    "    import pickle\n",
    "    # open a file, where you stored the pickled data\n",
    "    file = open(filename, 'rb')\n",
    "\n",
    "    # dump information to that file\n",
    "    output = pickle.load(file)\n",
    "    wav = output[wav_label]\n",
    "\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return wav\n",
    "\n",
    "wav_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'])) == 160000:\n",
    "        wav_list.append(load_wav_pkl(row['file']))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'])\n",
    "        print('short length: '+str(len(temp_wav)))\n",
    "        wav_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000\n",
    "\n",
    "wav_bg_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'], 'bg_y')) == 160000:\n",
    "        wav_bg_list.append(load_wav_pkl(row['file'], 'bg_y'))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'], 'bg_y')\n",
    "#         print('short length: '+str(len(temp_wav)))\n",
    "        wav_bg_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000\n",
    "\n",
    "wav_fg_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'], 'fg_y')) == 160000:\n",
    "        wav_fg_list.append(load_wav_pkl(row['file'], 'fg_y'))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'], 'fg_y')\n",
    "#         print('short length: '+str(len(temp_wav)))\n",
    "        wav_fg_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53798c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ds = tf.data.Dataset.from_tensor_slices((np.stack(wav_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_fg = tf.data.Dataset.from_tensor_slices((np.stack(wav_fg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_bg = tf.data.Dataset.from_tensor_slices((np.stack(wav_bg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3a63c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding(wav_data, label, fold):\n",
    "    # run YAMNet to extract embedding from the wav data\n",
    "    scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "    num_embeddings = tf.shape(embeddings)[0]\n",
    "    return (embeddings,\n",
    "            tf.repeat(label, num_embeddings),\n",
    "            tf.repeat(fold, num_embeddings))\n",
    "\n",
    "# extract embedding\n",
    "main_ds = main_ds.map(extract_embedding).unbatch()\n",
    "main_ds_fg = main_ds_fg.map(extract_embedding).unbatch()\n",
    "main_ds_bg = main_ds_bg.map(extract_embedding).unbatch()\n",
    "\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466f3c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ds_all3 = tf.data.Dataset.from_tensor_slices((np.stack(wav_list, axis = 0), np.stack(wav_bg_list, axis = 0), np.stack(wav_fg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_all3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23104af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(3072,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding_3(wav_data_raw, wav_data_bg, wav_data_fg, label, fold):\n",
    "    # run YAMNet to extract embedding from the wav data\n",
    "    scores, embeddings_raw, spectrogram = yamnet_model(wav_data_raw)\n",
    "    scores, embeddings_bg, spectrogram = yamnet_model(wav_data_bg)\n",
    "    scores, embeddings_fg, spectrogram = yamnet_model(wav_data_fg)\n",
    "    num_embeddings_raw = tf.shape(embeddings_raw)[0]\n",
    "    return (tf.concat([embeddings_raw, embeddings_bg, embeddings_fg],1),\n",
    "            tf.repeat(label, num_embeddings_raw),\n",
    "            tf.repeat(fold, num_embeddings_raw))\n",
    "\n",
    "# extract embedding\n",
    "main_ds_3 = main_ds_all3.map(extract_embedding_3).unbatch()\n",
    "\n",
    "main_ds_3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "439303be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andrewchang/opt/anaconda3/envs/AcousticEnv/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andrewchang/opt/anaconda3/envs/AcousticEnv/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "## raw signal\n",
    "cached_ds = main_ds.cache()\n",
    "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds = train_ds.map(remove_fold_column)\n",
    "val_ds = val_ds.map(remove_fold_column)\n",
    "test_ds = test_ds.map(remove_fold_column)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55f2c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## background sound\n",
    "cached_ds_bg = main_ds_bg.cache()\n",
    "train_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_bg = train_ds_bg.map(remove_fold_column)\n",
    "val_ds_bg = val_ds_bg.map(remove_fold_column)\n",
    "test_ds_bg = test_ds_bg.map(remove_fold_column)\n",
    "\n",
    "train_ds_bg = train_ds_bg.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_bg = val_ds_bg.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_bg = test_ds_bg.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "662d18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## foreground sound\n",
    "cached_ds_fg = main_ds_fg.cache()\n",
    "train_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_fg = train_ds_fg.map(remove_fold_column)\n",
    "val_ds_fg = val_ds_fg.map(remove_fold_column)\n",
    "test_ds_fg = test_ds_fg.map(remove_fold_column)\n",
    "\n",
    "train_ds_fg = train_ds_fg.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_fg = val_ds_fg.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_fg = test_ds_fg.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89879ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all 3 signals\n",
    "cached_ds_3 = main_ds_3.cache()\n",
    "train_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_3 = train_ds_3.map(remove_fold_column)\n",
    "val_ds_3 = val_ds_3.map(remove_fold_column)\n",
    "test_ds_3 = test_ds_3.map(remove_fold_column)\n",
    "\n",
    "train_ds_3 = train_ds_3.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_3 = val_ds_3.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_3 = test_ds_3.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe243be",
   "metadata": {},
   "source": [
    "# Model of raw signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efd43949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"raw_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(23)\n",
    "\n",
    "raw_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='raw_model')\n",
    "\n",
    "raw_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f392d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# callback will be used in the other models below too\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0259ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 171s 133ms/step - loss: 0.2733 - accuracy: 0.8932 - val_loss: 0.2810 - val_accuracy: 0.8874\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 7s 6ms/step - loss: 0.2255 - accuracy: 0.9117 - val_loss: 0.2987 - val_accuracy: 0.8817\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 7s 5ms/step - loss: 0.1958 - accuracy: 0.9224 - val_loss: 0.3295 - val_accuracy: 0.8915\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 8s 6ms/step - loss: 0.1787 - accuracy: 0.9311 - val_loss: 0.2966 - val_accuracy: 0.8964\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 8s 7ms/step - loss: 0.1586 - accuracy: 0.9390 - val_loss: 0.3128 - val_accuracy: 0.9013\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 8s 7ms/step - loss: 0.1470 - accuracy: 0.9453 - val_loss: 0.5040 - val_accuracy: 0.8845\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 9s 7ms/step - loss: 0.1450 - accuracy: 0.9511 - val_loss: 0.3413 - val_accuracy: 0.8949\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 9s 7ms/step - loss: 0.1219 - accuracy: 0.9563 - val_loss: 0.3542 - val_accuracy: 0.9002\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 9s 7ms/step - loss: 0.1114 - accuracy: 0.9614 - val_loss: 0.3939 - val_accuracy: 0.8909\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 8s 6ms/step - loss: 0.1105 - accuracy: 0.9646 - val_loss: 0.4285 - val_accuracy: 0.8847\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 7s 6ms/step - loss: 0.0894 - accuracy: 0.9692 - val_loss: 0.4161 - val_accuracy: 0.8877\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 8s 6ms/step - loss: 0.0874 - accuracy: 0.9715 - val_loss: 0.4292 - val_accuracy: 0.8870\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 8s 6ms/step - loss: 0.0889 - accuracy: 0.9737 - val_loss: 0.4862 - val_accuracy: 0.8881\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 8s 7ms/step - loss: 0.0813 - accuracy: 0.9772 - val_loss: 0.4654 - val_accuracy: 0.8904\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 8s 7ms/step - loss: 0.0683 - accuracy: 0.9794 - val_loss: 0.4824 - val_accuracy: 0.8917\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 7s 6ms/step - loss: 0.0620 - accuracy: 0.9811 - val_loss: 0.6037 - val_accuracy: 0.8828\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 8s 6ms/step - loss: 0.0737 - accuracy: 0.9816 - val_loss: 0.5246 - val_accuracy: 0.9009\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 8s 6ms/step - loss: 0.0625 - accuracy: 0.9838 - val_loss: 0.5674 - val_accuracy: 0.8957\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 9s 7ms/step - loss: 0.0875 - accuracy: 0.9837 - val_loss: 0.6344 - val_accuracy: 0.8785\n"
     ]
    }
   ],
   "source": [
    "history = raw_model.fit(train_ds,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d05995ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 2s 6ms/step - loss: 0.7735 - accuracy: 0.8443\n",
      "Loss:  0.7735467553138733\n",
      "Accuracy:  0.844268798828125\n"
     ]
    }
   ],
   "source": [
    "# this is the accuracy per YAMNet sample, not the accuracy per audio file\n",
    "loss, accuracy = raw_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ec330d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_by_audio(model, test_ds, y_true):\n",
    "    n_sample_per_audio = 20\n",
    "    y_pred = model.predict(test_ds)\n",
    "    y_reshaped = y_pred.reshape(-1, n_sample_per_audio, 2) # reshape to audio*sample*binary_prediction\n",
    "    y_pred_by_row = y_reshaped.mean(axis=1).argmax(axis=1)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(y_true, y_pred_by_row), y_pred_by_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db50fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 2ms/step\n",
      "0.8814229249011858\n"
     ]
    }
   ],
   "source": [
    "test_df = df_all[df_all['fold']==9]\n",
    "acc, y_pred = acc_by_audio(raw_model, test_ds, y_true = test_df['category'])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "552b07f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy by audio source\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/slnm06yn15lgwlcg_7jpt3t00000gn/T/ipykernel_82392/2884005967.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['pred'] = y_pred\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "source\n",
       "city_GoogleAudioSet                                0.578947\n",
       "city_SONYC                                         0.971831\n",
       "city_youtube                                       0.700000\n",
       "nature_AmbisonicSoundLibrary/nature                0.750000\n",
       "nature_GoogleAudioSet/Outside, rural or natural    0.461538\n",
       "nature_S2L_LULC/non_urban                          1.000000\n",
       "nature_S2L_LULC/urban_0_25                         1.000000\n",
       "nature_youtube/NatureSoundscapes                   1.000000\n",
       "nature_youtube/NomadicAmbience_nature              0.800000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_df['pred'] = y_pred\n",
    "print('accuracy by audio source')\n",
    "test_df.groupby(['source']).apply(lambda x: accuracy_score(x['category'], x['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240d839",
   "metadata": {},
   "source": [
    "# Model of background signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40f53bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bg_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(23)\n",
    "\n",
    "bg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='bg_model')\n",
    "\n",
    "bg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acc05654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 215s 169ms/step - loss: 0.2707 - accuracy: 0.8940 - val_loss: 0.2812 - val_accuracy: 0.8862\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 8s 6ms/step - loss: 0.2192 - accuracy: 0.9109 - val_loss: 0.3201 - val_accuracy: 0.8719\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 8s 7ms/step - loss: 0.1926 - accuracy: 0.9233 - val_loss: 0.3267 - val_accuracy: 0.8749\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 9s 7ms/step - loss: 0.1714 - accuracy: 0.9314 - val_loss: 0.3239 - val_accuracy: 0.8898\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 8s 7ms/step - loss: 0.1527 - accuracy: 0.9393 - val_loss: 0.3234 - val_accuracy: 0.8985\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 8s 7ms/step - loss: 0.1364 - accuracy: 0.9468 - val_loss: 0.3402 - val_accuracy: 0.8964\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 8s 7ms/step - loss: 0.1215 - accuracy: 0.9530 - val_loss: 0.3787 - val_accuracy: 0.8887\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 9s 7ms/step - loss: 0.1106 - accuracy: 0.9586 - val_loss: 0.4015 - val_accuracy: 0.8883\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.1058 - accuracy: 0.9618 - val_loss: 0.4345 - val_accuracy: 0.8913\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 5s 4ms/step - loss: 0.0923 - accuracy: 0.9673 - val_loss: 0.4638 - val_accuracy: 0.8855\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.0948 - accuracy: 0.9703 - val_loss: 0.4941 - val_accuracy: 0.8919\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 6s 4ms/step - loss: 0.0770 - accuracy: 0.9729 - val_loss: 0.5075 - val_accuracy: 0.8840\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.0727 - accuracy: 0.9744 - val_loss: 0.4762 - val_accuracy: 0.8943\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.0616 - accuracy: 0.9777 - val_loss: 0.5584 - val_accuracy: 0.8830\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.0596 - accuracy: 0.9806 - val_loss: 0.5060 - val_accuracy: 0.8919\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 7s 5ms/step - loss: 0.0515 - accuracy: 0.9822 - val_loss: 0.5216 - val_accuracy: 0.8921\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.0513 - accuracy: 0.9833 - val_loss: 0.5819 - val_accuracy: 0.8947\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 7s 6ms/step - loss: 0.0474 - accuracy: 0.9844 - val_loss: 0.6043 - val_accuracy: 0.8911\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 8s 6ms/step - loss: 0.0443 - accuracy: 0.9844 - val_loss: 0.6384 - val_accuracy: 0.8947\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 8s 6ms/step - loss: 0.0413 - accuracy: 0.9865 - val_loss: 0.6237 - val_accuracy: 0.8989\n"
     ]
    }
   ],
   "source": [
    "bg_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = bg_model.fit(train_ds_bg,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_bg,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "298a1656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 2s 6ms/step - loss: 0.9467 - accuracy: 0.8502\n",
      "Loss:  0.9466601014137268\n",
      "Accuracy:  0.8501976132392883\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = bg_model.evaluate(test_ds_bg)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "065702ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 1ms/step\n",
      "0.8814229249011858\n"
     ]
    }
   ],
   "source": [
    "test_df = df_all[df_all['fold']==9]\n",
    "acc, y_pred = acc_by_audio(bg_model, test_ds_bg, y_true = test_df['category'])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfe35fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy by audio source\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/slnm06yn15lgwlcg_7jpt3t00000gn/T/ipykernel_82392/2884005967.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['pred'] = y_pred\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "source\n",
       "city_GoogleAudioSet                                0.736842\n",
       "city_SONYC                                         0.985915\n",
       "city_youtube                                       0.600000\n",
       "nature_AmbisonicSoundLibrary/nature                0.625000\n",
       "nature_GoogleAudioSet/Outside, rural or natural    0.538462\n",
       "nature_S2L_LULC/non_urban                          1.000000\n",
       "nature_S2L_LULC/urban_0_25                         1.000000\n",
       "nature_youtube/NatureSoundscapes                   1.000000\n",
       "nature_youtube/NomadicAmbience_nature              0.700000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_df['pred'] = y_pred\n",
    "print('accuracy by audio source')\n",
    "test_df.groupby(['source']).apply(lambda x: accuracy_score(x['category'], x['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5dd27",
   "metadata": {},
   "source": [
    "# Model of foreground signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c377256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fg_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(23)\n",
    "\n",
    "fg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='fg_model')\n",
    "\n",
    "fg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57973000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1247/1247 [==============================] - 167s 131ms/step - loss: 0.5392 - accuracy: 0.7384 - val_loss: 0.5302 - val_accuracy: 0.7209\n",
      "Epoch 2/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.4930 - accuracy: 0.7582 - val_loss: 0.4972 - val_accuracy: 0.7428\n",
      "Epoch 3/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.4778 - accuracy: 0.7679 - val_loss: 0.5003 - val_accuracy: 0.7400\n",
      "Epoch 4/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.4578 - accuracy: 0.7786 - val_loss: 0.5113 - val_accuracy: 0.7451\n",
      "Epoch 5/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.4451 - accuracy: 0.7856 - val_loss: 0.5138 - val_accuracy: 0.7383\n",
      "Epoch 6/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.4302 - accuracy: 0.7972 - val_loss: 0.5023 - val_accuracy: 0.7621\n",
      "Epoch 7/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.4168 - accuracy: 0.8042 - val_loss: 0.5001 - val_accuracy: 0.7602\n",
      "Epoch 8/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.4023 - accuracy: 0.8116 - val_loss: 0.5082 - val_accuracy: 0.7557\n",
      "Epoch 9/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.3880 - accuracy: 0.8187 - val_loss: 0.5472 - val_accuracy: 0.7385\n",
      "Epoch 10/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.3744 - accuracy: 0.8283 - val_loss: 0.5283 - val_accuracy: 0.7604\n",
      "Epoch 11/20\n",
      "1247/1247 [==============================] - 6s 4ms/step - loss: 0.3612 - accuracy: 0.8349 - val_loss: 0.5506 - val_accuracy: 0.7581\n",
      "Epoch 12/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.3467 - accuracy: 0.8434 - val_loss: 0.5762 - val_accuracy: 0.7496\n",
      "Epoch 13/20\n",
      "1247/1247 [==============================] - 6s 5ms/step - loss: 0.3311 - accuracy: 0.8499 - val_loss: 0.6111 - val_accuracy: 0.7457\n",
      "Epoch 14/20\n",
      "1247/1247 [==============================] - 5s 4ms/step - loss: 0.3181 - accuracy: 0.8570 - val_loss: 0.5977 - val_accuracy: 0.7587\n",
      "Epoch 15/20\n",
      "1247/1247 [==============================] - 5s 4ms/step - loss: 0.3045 - accuracy: 0.8649 - val_loss: 0.6287 - val_accuracy: 0.7662\n",
      "Epoch 16/20\n",
      "1247/1247 [==============================] - 5s 4ms/step - loss: 0.2911 - accuracy: 0.8720 - val_loss: 0.6715 - val_accuracy: 0.7421\n",
      "Epoch 17/20\n",
      "1247/1247 [==============================] - 5s 4ms/step - loss: 0.2801 - accuracy: 0.8777 - val_loss: 0.6602 - val_accuracy: 0.7466\n",
      "Epoch 18/20\n",
      "1247/1247 [==============================] - 5s 4ms/step - loss: 0.2656 - accuracy: 0.8844 - val_loss: 0.7084 - val_accuracy: 0.7419\n",
      "Epoch 19/20\n",
      "1247/1247 [==============================] - 5s 4ms/step - loss: 0.2535 - accuracy: 0.8899 - val_loss: 0.7404 - val_accuracy: 0.7383\n",
      "Epoch 20/20\n",
      "1247/1247 [==============================] - 5s 4ms/step - loss: 0.2401 - accuracy: 0.8966 - val_loss: 0.7263 - val_accuracy: 0.7411\n"
     ]
    }
   ],
   "source": [
    "fg_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = fg_model.fit(train_ds_fg,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_fg,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fc48ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 2s 6ms/step - loss: 0.8671 - accuracy: 0.7006\n",
      "Loss:  0.867089569568634\n",
      "Accuracy:  0.7005928754806519\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = fg_model.evaluate(test_ds_fg)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38a7902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 2ms/step\n",
      "0.83399209486166\n"
     ]
    }
   ],
   "source": [
    "test_df = df_all[df_all['fold']==9]\n",
    "acc, y_pred = acc_by_audio(fg_model, test_ds_fg, y_true = test_df['category'])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed76976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy by audio source\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/slnm06yn15lgwlcg_7jpt3t00000gn/T/ipykernel_82392/2884005967.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['pred'] = y_pred\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "source\n",
       "city_GoogleAudioSet                                0.684211\n",
       "city_SONYC                                         0.887324\n",
       "city_youtube                                       0.500000\n",
       "nature_AmbisonicSoundLibrary/nature                0.875000\n",
       "nature_GoogleAudioSet/Outside, rural or natural    0.461538\n",
       "nature_S2L_LULC/non_urban                          1.000000\n",
       "nature_S2L_LULC/urban_0_25                         0.916667\n",
       "nature_youtube/NatureSoundscapes                   0.981818\n",
       "nature_youtube/NomadicAmbience_nature              0.800000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_df['pred'] = y_pred\n",
    "print('accuracy by audio source')\n",
    "test_df.groupby(['source']).apply(lambda x: accuracy_score(x['category'], x['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f696c",
   "metadata": {},
   "source": [
    "# Model of 3 signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2e3e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_classes = ['city', 'nature']\n",
    "# tf.keras.backend.clear_session()\n",
    "# tf.random.seta_seed(23)\n",
    "\n",
    "# all3_model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(3072), dtype=tf.float32,\n",
    "#                           name='input_embedding'),\n",
    "#     tf.keras.layers.Dense(1024, activation='relu'),\n",
    "# #     tf.keras.layers.Dense(1024, activation='relu'),\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),\n",
    "#     tf.keras.layers.Dense(len(my_classes))\n",
    "# ], name='all3_model')\n",
    "\n",
    "# all3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f113020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all3_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#                     optimizer=\"adam\",\n",
    "#                     metrics=['accuracy'])\n",
    "\n",
    "# # callback will be used in the other models below too\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "#                                             patience=3,\n",
    "#                                             restore_best_weights=True)\n",
    "\n",
    "\n",
    "# history = all3_model.fit(train_ds_3,\n",
    "#                        epochs=20,\n",
    "#                        validation_data=val_ds_3,\n",
    "#                        callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13af5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accuracy = all3_model.evaluate(test_ds_3)\n",
    "\n",
    "# print(\"Loss: \", loss)\n",
    "# print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0660f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = df_all[df_all['fold']==9]\n",
    "# acc = acc_by_audio(all3_model, test_ds_3, y_true = test_df['category'])\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0db323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
